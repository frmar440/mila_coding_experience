{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Band Name</th>\n",
       "      <th>Band Genre</th>\n",
       "      <th>Band Country of Origin</th>\n",
       "      <th>Band Debut</th>\n",
       "      <th>Concert ID</th>\n",
       "      <th>Concert Attendance</th>\n",
       "      <th>Inside Venue</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Seated</th>\n",
       "      <th>Personnality Trait 1</th>\n",
       "      <th>Personnality Trait 2</th>\n",
       "      <th>Personnality Trait 3</th>\n",
       "      <th>Personnality Trait 4</th>\n",
       "      <th>Concert Goer Age</th>\n",
       "      <th>Concert Goer ID</th>\n",
       "      <th>Height (cm)</th>\n",
       "      <th>Concert Goer Country of Origin</th>\n",
       "      <th>Concert Enjoyment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ConcertExperience_180106</td>\n",
       "      <td>Teenage Crazy Blue Knickers</td>\n",
       "      <td>Indie/Alt Rock</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2980.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.330843</td>\n",
       "      <td>-0.958408</td>\n",
       "      <td>-0.943548</td>\n",
       "      <td>-1.636806</td>\n",
       "      <td>29.0</td>\n",
       "      <td>concert_goer_1985</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Paraguay</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ConcertExperience_146268</td>\n",
       "      <td>Beyond Devon</td>\n",
       "      <td>Pop Music</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>731.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-2.069449</td>\n",
       "      <td>0.017777</td>\n",
       "      <td>-1.910675</td>\n",
       "      <td>0.610265</td>\n",
       "      <td>43.0</td>\n",
       "      <td>concert_goer_1874</td>\n",
       "      <td>158.0</td>\n",
       "      <td>United Kingdom (UK)</td>\n",
       "      <td>Enjoyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ConcertExperience_128743</td>\n",
       "      <td>Ron Talent</td>\n",
       "      <td>Rock n Roll</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162754.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.484268</td>\n",
       "      <td>1.968772</td>\n",
       "      <td>-0.064167</td>\n",
       "      <td>-1.260871</td>\n",
       "      <td>68.0</td>\n",
       "      <td>concert_goer_442</td>\n",
       "      <td>159.0</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ConcertExperience_140839</td>\n",
       "      <td>Devon Revival</td>\n",
       "      <td>RnB</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>8103.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.858054</td>\n",
       "      <td>1.022827</td>\n",
       "      <td>-0.348389</td>\n",
       "      <td>-1.147251</td>\n",
       "      <td>17.0</td>\n",
       "      <td>concert_goer_1149</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Worst Concert Ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ConcertExperience_19149</td>\n",
       "      <td>Beyond Devon</td>\n",
       "      <td>Pop Music</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.793029</td>\n",
       "      <td>-1.166528</td>\n",
       "      <td>-0.043766</td>\n",
       "      <td>0.969661</td>\n",
       "      <td>59.0</td>\n",
       "      <td>concert_goer_930</td>\n",
       "      <td>166.0</td>\n",
       "      <td>United Kingdom (UK)</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169995</th>\n",
       "      <td>ConcertExperience_14055</td>\n",
       "      <td>Crazy Joystick Cult</td>\n",
       "      <td>RnB</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>162754.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.095021</td>\n",
       "      <td>0.175175</td>\n",
       "      <td>0.914245</td>\n",
       "      <td>0.357359</td>\n",
       "      <td>50.0</td>\n",
       "      <td>concert_goer_707</td>\n",
       "      <td>180.0</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169996</th>\n",
       "      <td>ConcertExperience_192792</td>\n",
       "      <td>Crazy Joystick Cult</td>\n",
       "      <td>RnB</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>963.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.733719</td>\n",
       "      <td>-0.285776</td>\n",
       "      <td>-0.323312</td>\n",
       "      <td>0.641180</td>\n",
       "      <td>71.0</td>\n",
       "      <td>concert_goer_1373</td>\n",
       "      <td>143.0</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>Worst Concert Ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169997</th>\n",
       "      <td>ConcertExperience_152942</td>\n",
       "      <td>Why Frogs, Why?</td>\n",
       "      <td>Heavy Metal</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>764.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.744969</td>\n",
       "      <td>-0.965547</td>\n",
       "      <td>1.020598</td>\n",
       "      <td>1.027389</td>\n",
       "      <td>27.0</td>\n",
       "      <td>concert_goer_1286</td>\n",
       "      <td>176.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169998</th>\n",
       "      <td>ConcertExperience_138957</td>\n",
       "      <td>Twilight of the Joystick Gods</td>\n",
       "      <td>Hip Hop/Rap</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>22026.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.821976</td>\n",
       "      <td>0.351411</td>\n",
       "      <td>0.175762</td>\n",
       "      <td>1.455654</td>\n",
       "      <td>39.0</td>\n",
       "      <td>concert_goer_1845</td>\n",
       "      <td>176.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169999</th>\n",
       "      <td>ConcertExperience_132336</td>\n",
       "      <td>Joystick Attack</td>\n",
       "      <td>Heavy Metal</td>\n",
       "      <td>United Kingdom (UK)</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>2980.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.017410</td>\n",
       "      <td>0.883248</td>\n",
       "      <td>-0.184107</td>\n",
       "      <td>0.631731</td>\n",
       "      <td>46.0</td>\n",
       "      <td>concert_goer_1281</td>\n",
       "      <td>146.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Best Concert Ever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Id                      Band Name  \\\n",
       "0       ConcertExperience_180106    Teenage Crazy Blue Knickers   \n",
       "1       ConcertExperience_146268                   Beyond Devon   \n",
       "2       ConcertExperience_128743                     Ron Talent   \n",
       "3       ConcertExperience_140839                  Devon Revival   \n",
       "4        ConcertExperience_19149                   Beyond Devon   \n",
       "...                          ...                            ...   \n",
       "169995   ConcertExperience_14055            Crazy Joystick Cult   \n",
       "169996  ConcertExperience_192792            Crazy Joystick Cult   \n",
       "169997  ConcertExperience_152942                Why Frogs, Why?   \n",
       "169998  ConcertExperience_138957  Twilight of the Joystick Gods   \n",
       "169999  ConcertExperience_132336                Joystick Attack   \n",
       "\n",
       "            Band Genre          Band Country of Origin  Band Debut  \\\n",
       "0       Indie/Alt Rock  United States of America (USA)      1976.0   \n",
       "1            Pop Music  United States of America (USA)      1968.0   \n",
       "2          Rock n Roll                          Canada      1955.0   \n",
       "3                  RnB  United States of America (USA)      1992.0   \n",
       "4            Pop Music  United States of America (USA)      1968.0   \n",
       "...                ...                             ...         ...   \n",
       "169995             RnB                          Canada      1985.0   \n",
       "169996             RnB                          Canada      1985.0   \n",
       "169997     Heavy Metal                          Canada      2005.0   \n",
       "169998     Hip Hop/Rap  United States of America (USA)      1995.0   \n",
       "169999     Heavy Metal             United Kingdom (UK)      2008.0   \n",
       "\n",
       "        Concert ID  Concert Attendance Inside Venue   Rain Seated  \\\n",
       "0            900.0              2980.0        False  False    NaN   \n",
       "1            731.0                54.0         True  False   True   \n",
       "2              NaN            162754.0        False  False   True   \n",
       "3            704.0              8103.0        False   True  False   \n",
       "4             95.0                54.0        False  False  False   \n",
       "...            ...                 ...          ...    ...    ...   \n",
       "169995        70.0            162754.0         True  False  False   \n",
       "169996       963.0                54.0        False  False  False   \n",
       "169997       764.0                54.0        False  False  False   \n",
       "169998       694.0             22026.0        False   True   True   \n",
       "169999       661.0              2980.0         True  False  False   \n",
       "\n",
       "        Personnality Trait 1  Personnality Trait 2  Personnality Trait 3  \\\n",
       "0                   0.330843             -0.958408             -0.943548   \n",
       "1                  -2.069449              0.017777             -1.910675   \n",
       "2                  -0.484268              1.968772             -0.064167   \n",
       "3                  -0.858054              1.022827             -0.348389   \n",
       "4                  -0.793029             -1.166528             -0.043766   \n",
       "...                      ...                   ...                   ...   \n",
       "169995             -0.095021              0.175175              0.914245   \n",
       "169996             -0.733719             -0.285776             -0.323312   \n",
       "169997              0.744969             -0.965547              1.020598   \n",
       "169998              0.821976              0.351411              0.175762   \n",
       "169999             -1.017410              0.883248             -0.184107   \n",
       "\n",
       "        Personnality Trait 4  Concert Goer Age    Concert Goer ID  \\\n",
       "0                  -1.636806              29.0  concert_goer_1985   \n",
       "1                   0.610265              43.0  concert_goer_1874   \n",
       "2                  -1.260871              68.0   concert_goer_442   \n",
       "3                  -1.147251              17.0  concert_goer_1149   \n",
       "4                   0.969661              59.0   concert_goer_930   \n",
       "...                      ...               ...                ...   \n",
       "169995              0.357359              50.0   concert_goer_707   \n",
       "169996              0.641180              71.0  concert_goer_1373   \n",
       "169997              1.027389              27.0  concert_goer_1286   \n",
       "169998              1.455654              39.0  concert_goer_1845   \n",
       "169999              0.631731              46.0  concert_goer_1281   \n",
       "\n",
       "        Height (cm)  Concert Goer Country of Origin   Concert Enjoyment  \n",
       "0             140.0                        Paraguay       Did Not Enjoy  \n",
       "1             158.0             United Kingdom (UK)             Enjoyed  \n",
       "2             159.0  United States of America (USA)       Did Not Enjoy  \n",
       "3             150.0                          Canada  Worst Concert Ever  \n",
       "4             166.0             United Kingdom (UK)       Did Not Enjoy  \n",
       "...             ...                             ...                 ...  \n",
       "169995        180.0  United States of America (USA)       Did Not Enjoy  \n",
       "169996        143.0                        Bulgaria  Worst Concert Ever  \n",
       "169997        176.0                          Canada       Did Not Enjoy  \n",
       "169998        176.0                          Canada       Did Not Enjoy  \n",
       "169999        146.0                          Canada   Best Concert Ever  \n",
       "\n",
       "[170000 rows x 19 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data statistics\n",
    "value = train_data.mode().loc[0] # most frequent strategy\n",
    "\n",
    "# fill nan values\n",
    "train_data.fillna(value=value, inplace=True)\n",
    "test_data.fillna(value=value, inplace=True)\n",
    "\n",
    "# convert Concert ID to string\n",
    "train_data['Concert ID'] = train_data['Concert ID'].map(lambda x: str(x))\n",
    "test_data['Concert ID'] = test_data['Concert ID'].map(lambda x: str(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target and ID transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = {\n",
    "    'Worst Concert Ever': 0,\n",
    "    'Did Not Enjoy': 1,\n",
    "    'Enjoyed': 2,\n",
    "    'Best Concert Ever': 3\n",
    "}\n",
    "\n",
    "target_inverse_transform = {\n",
    "    0: 'Worst Concert Ever',\n",
    "    1: 'Did Not Enjoy',\n",
    "    2: 'Enjoyed',\n",
    "    3: 'Best Concert Ever'\n",
    "}\n",
    "\n",
    "train_data['Concert Enjoyment'] = train_data['Concert Enjoyment'].map(target_transform)\n",
    "\n",
    "# fit custom transforms on train data\n",
    "concert_id_transform = train_data[['Concert ID', 'Concert Enjoyment']].groupby(['Concert ID']).mean()['Concert Enjoyment']\n",
    "concert_goer_id_transform = train_data[['Concert Goer ID', 'Concert Enjoyment']].groupby(['Concert Goer ID']).mean()['Concert Enjoyment']\n",
    "\n",
    "# apply custom transforms on train and test data\n",
    "train_data['Concert ID'] = train_data['Concert ID'].map(concert_id_transform)\n",
    "train_data['Concert Goer ID'] = train_data['Concert Goer ID'].map(concert_goer_id_transform)\n",
    "\n",
    "test_data['Concert ID'] = test_data['Concert ID'].map(concert_id_transform)\n",
    "test_data['Concert Goer ID'] = test_data['Concert Goer ID'].map(concert_goer_id_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "# instantiate transformers\n",
    "mlb = MultiLabelBinarizer()\n",
    "mms = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "\n",
    "# fit transformers on train data\n",
    "mlb.fit( train_data[['Band Name', 'Band Genre', 'Band Country of Origin', 'Concert Goer Country of Origin']].to_numpy() )\n",
    "mms.fit( train_data[['Band Rating', 'Band Debut', 'Concert ID', 'Concert Attendance', 'Concert Goer Age', 'Concert Goer ID', 'Height (cm)']].to_numpy() )\n",
    "ss.fit( train_data[['Personnality Trait 1', 'Personnality Trait 2', 'Personnality Trait 3', 'Personnality Trait 4']].to_numpy() )\n",
    "\n",
    "\n",
    "# apply transformers on train data\n",
    "X_train = np.hstack((\n",
    "    mlb.transform( train_data[['Band Name', 'Band Genre', 'Band Country of Origin', 'Concert Goer Country of Origin']].to_numpy() ),\n",
    "    mms.transform( train_data[['Band Rating', 'Band Debut', 'Concert ID', 'Concert Attendance', 'Concert Goer Age', 'Concert Goer ID', 'Height (cm)']].to_numpy() ),\n",
    "    ss.transform( train_data[['Personnality Trait 1', 'Personnality Trait 2', 'Personnality Trait 3', 'Personnality Trait 4']].to_numpy() ),\n",
    "    train_data[['Inside Venue', 'Rain', 'Seated']].to_numpy(dtype=int)\n",
    "))\n",
    "\n",
    "y_train = train_data['Concert Enjoyment'].to_numpy()\n",
    "\n",
    "\n",
    "# apply transformers on test data\n",
    "X_test = np.hstack((\n",
    "    mlb.transform( test_data[['Band Name', 'Band Genre', 'Band Country of Origin', 'Concert Goer Country of Origin']].to_numpy() ),\n",
    "    mms.transform( test_data[['Band Rating', 'Band Debut', 'Concert ID', 'Concert Attendance', 'Concert Goer Age', 'Concert Goer ID', 'Height (cm)']].to_numpy() ),\n",
    "    ss.transform( test_data[['Personnality Trait 1', 'Personnality Trait 2', 'Personnality Trait 3', 'Personnality Trait 4']].to_numpy() ),\n",
    "    test_data[['Inside Venue', 'Rain', 'Seated']].to_numpy(dtype=int)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 0.9:0.1 (train:validation) split for model validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcertDataset(Dataset):\n",
    "    \"\"\"Store the samples and their corresponding labels\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Numpy array of shape [n_samples, n_features]\n",
    "            y (np.ndarray): Numpy array of shape [n_samples]\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Length special method\n",
    "\n",
    "        Returns:\n",
    "            int: length of dataset\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"Get item special method\n",
    "\n",
    "        Args:\n",
    "            idx (int): index\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor]: sample features of shape [n_features] and label of shape [1]\n",
    "        \"\"\"\n",
    "        features = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long) # type long required for CrossEntropyLoss\n",
    "        return features, label\n",
    "\n",
    "\n",
    "# instantiate datasets\n",
    "train_dataset = ConcertDataset(X_train, y_train)\n",
    "val_dataset = ConcertDataset(X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # discriminative\n",
    "from sklearn.svm import LinearSVC # discriminant-based\n",
    "from sklearn.tree import DecisionTreeClassifier # decision tree\n",
    "\n",
    "\n",
    "class FNNClassifier(nn.Module):\n",
    "    \"\"\"Feedforward neural network with three layers\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, num_nodes: int, out_features: int) -> None:\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            in_features (int): number of features\n",
    "            num_nodes (int): number of hidden nodes\n",
    "            out_features (int): number of classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features, num_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_nodes, num_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_nodes, out_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Torch tensor of shape [batch_size, n_features]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Torch tensor of shape [batch_size, 4]\n",
    "        \"\"\"\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "logistic_classifier = LogisticRegression(penalty='l2', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter': [1000, 10000]\n",
    "}\n",
    "\n",
    "# k-fold cross-validator\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# hyper-parameter optimizer\n",
    "logistic_classifier_grid_search = GridSearchCV(\n",
    "    estimator=logistic_classifier,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=k_fold\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "logistic_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred_logistic_classifier = logistic_classifier_grid_search.predict(X_val)\n",
    "\n",
    "# compute validation accuracy\n",
    "accuracy_logistic_classifier = accuracy_score(y_val, y_pred_logistic_classifier)\n",
    "\n",
    "# save grid search\n",
    "joblib.dump(logistic_classifier_grid_search, f'storage/logistic_classifier_acc{accuracy_logistic_classifier:.5f}.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_svc = LinearSVC(penalty='l2', loss='squared_hinge', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter': [1000, 10000]\n",
    "}\n",
    "\n",
    "# k-fold cross-validator\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# hyper-parameter optimizer\n",
    "linear_svc_grid_search = GridSearchCV(\n",
    "    estimator=linear_svc,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=k_fold\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "linear_svc_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred_linear_svc = linear_svc_grid_search.predict(X_val)\n",
    "\n",
    "# compute validation accuracy\n",
    "accuracy_linear_svc = accuracy_score(y_val, y_pred_linear_svc)\n",
    "\n",
    "# save grid search\n",
    "joblib.dump(linear_svc_grid_search, f'storage/linear_svc_acc{accuracy_linear_svc:.5f}.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "# k-fold cross-validator\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# hyper-parameter optimizer\n",
    "decision_tree_classifier_grid_search = GridSearchCV(\n",
    "    estimator=decision_tree_classifier,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=k_fold\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "decision_tree_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred_decision_tree_classifier = decision_tree_classifier_grid_search.predict(X_val)\n",
    "\n",
    "# compute validation accuracy\n",
    "accuracy_decision_tree_classifier = accuracy_score(y_val, y_pred_decision_tree_classifier)\n",
    "\n",
    "# save grid search\n",
    "joblib.dump(decision_tree_classifier_grid_search, f'storage/decision_tree_classifier_acc{accuracy_decision_tree_classifier:.5f}.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validation(val_dataloader: DataLoader, model: nn.Module, loss_fn: nn.CrossEntropyLoss) -> float:\n",
    "    \"\"\"Validation loop\n",
    "\n",
    "    Args:\n",
    "        val_dataloader (DataLoader)\n",
    "        model (nn.Module)\n",
    "        loss_fn (nn.CrossEntropyLoss)\n",
    "\n",
    "    Returns:\n",
    "        float: val loss\n",
    "    \"\"\"\n",
    "    size = len(val_dataloader.dataset)\n",
    "    num_batches = len(val_dataloader)\n",
    "    val_loss = 0.\n",
    "    correct = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_dataloader:\n",
    "            outputs = model(features)\n",
    "            val_loss += loss_fn(outputs, labels).item()\n",
    "            correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n",
    "    \n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation Error:\\n    Accuracy: {(100*correct):>.2f}%\\n    Loss: {val_loss:>.8f}\\n\")\n",
    "    return val_loss, correct\n",
    "\n",
    "\n",
    "def epoch(train_dataloader: DataLoader, val_dataloader: DataLoader, \n",
    "            model: nn.Module, loss_fn: nn.CrossEntropyLoss, optimizer: torch.optim.Adam) -> Tuple:\n",
    "    \"\"\"Train one epoch\n",
    "\n",
    "    Args:\n",
    "        train_dataloader (DataLoader)\n",
    "        val_dataloader (DataLoader)\n",
    "        model (nn.Module)\n",
    "        loss_fn (nn.CrossEntropyLoss)\n",
    "        optimizer (torch.optim.Adam)\n",
    "\n",
    "    Returns:\n",
    "        Tuple\n",
    "    \"\"\"\n",
    "    epoch_train_losses = []\n",
    "    epoch_train_steps = []\n",
    "\n",
    "    size = len(train_dataloader.dataset)\n",
    "\n",
    "    for step, (features, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss = loss.item()\n",
    "\n",
    "        epoch_train_losses.append(loss)\n",
    "        epoch_train_steps.append(step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            current = step * len(features)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "        if step == 0:\n",
    "\n",
    "            # forward pass\n",
    "            val_loss, correct = validation(val_dataloader, model, loss_fn)\n",
    "        \n",
    "            epoch_val_loss = val_loss\n",
    "            epoch_val_accuracy = correct\n",
    "    \n",
    "    return epoch_train_losses, epoch_train_steps, epoch_val_loss, epoch_val_accuracy\n",
    "\n",
    "\n",
    "def train(num_epochs: int, train_dataloader: DataLoader, val_dataloader: DataLoader, \n",
    "            model: nn.Module, loss_fn: nn.MSELoss, optimizer: torch.optim.Adam) -> tuple:\n",
    "    \"\"\"Train loop\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int)\n",
    "        train_dataloader (DataLoader)\n",
    "        val_dataloader (DataLoader)\n",
    "        model (nn.Module)\n",
    "        loss_fn (nn.MSELoss)\n",
    "        optimizer (torch.optim.Adam)\n",
    "\n",
    "    Returns:\n",
    "        tuple\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    train_steps = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    val_steps = []\n",
    "    epoch_last_step = 0\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    for t in range(num_epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "        epoch_train_losses, epoch_train_steps, \\\n",
    "            epoch_val_loss, epoch_val_accuracy = \\\n",
    "                epoch(train_dataloader, val_dataloader, model, loss_fn, optimizer)\n",
    "        \n",
    "        if epoch_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = epoch_val_accuracy\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "        train_losses += epoch_train_losses\n",
    "        train_steps += [step + epoch_last_step for step in epoch_train_steps]\n",
    "\n",
    "        val_losses.append( epoch_val_loss )\n",
    "        val_accuracies.append( epoch_val_accuracy )\n",
    "        val_steps.append( epoch_last_step )\n",
    "\n",
    "        epoch_last_step = train_steps[-1]\n",
    "\n",
    "    return train_losses, train_steps, val_losses, val_accuracies, val_steps, best_val_accuracy, best_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FNN hyper-parameters\n",
    "NODES = [32, 64, 128]\n",
    "\n",
    "# training hyper-parameters\n",
    "BATCH_SIZES = [64, 128, 256]\n",
    "LEARNING_RATES = [1e-5, 1e-4, 1e-3]\n",
    "N_EPOCHS = 30\n",
    "\n",
    "# grid search\n",
    "for NODE in NODES:\n",
    "    for BATCH_SIZE in BATCH_SIZES:\n",
    "        for LEARNING_RATE in LEARNING_RATES:\n",
    "\n",
    "            # DataLoader wraps an iterable around the Dataset\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "            model = FNNClassifier(in_features=X_train.shape[1], num_nodes=NODE, out_features=4).to('cpu')\n",
    "\n",
    "\n",
    "            loss_fn = nn.CrossEntropyLoss() # negative log likelihood of softmax function\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "            # train model\n",
    "            train_losses, train_steps, val_losses, val_accuracies, val_steps, best_val_accuracy, best_state = \\\n",
    "                train(N_EPOCHS, train_dataloader, val_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "\n",
    "            # save model weights\n",
    "            torch.save(best_state, f'storage/FNN_{NODE}_bs{BATCH_SIZE}_lr{LEARNING_RATE}_ep{N_EPOCHS}_acc{best_val_accuracy:.5f}.pth')\n",
    "\n",
    "\n",
    "            # save train stats\n",
    "            train_stats = {   \n",
    "                'train_steps': train_steps,\n",
    "                'train_losses': train_losses,\n",
    "                'val_steps': val_steps,\n",
    "                'val_accuracies': val_accuracies,\n",
    "                'val_losses': val_losses\n",
    "            }\n",
    "\n",
    "            with open(f'storage/FNN_{NODE}_bs{BATCH_SIZE}_lr{LEARNING_RATE}_ep{N_EPOCHS}_acc{best_val_accuracy:.5f}.pkl', 'wb') as handle:\n",
    "                pickle.dump(train_stats, handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NODES = 1024\n",
    "\n",
    "model = FNNClassifier(in_features=X_train.shape[1], num_nodes=N_NODES, out_features=4)\n",
    "model.load_state_dict( torch.load('storage/FNN3_1024_bs256_lr0.0001_ep10_acc0.68035.pth') )\n",
    "model.eval()\n",
    "\n",
    "outputs = model( torch.tensor(X_test, dtype=torch.float32) )\n",
    "predictions = le.inverse_transform( outputs.argmax(axis=1) )\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_data['Id'],\n",
    "    'Predicted': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submissions/FNN3_1024_bs256_lr0.0001_ep10_acc0.68035.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e3b4952062f618dbf0490f0178ad443c32c089b4a05be38315e03675ee3253a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
